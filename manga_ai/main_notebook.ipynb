{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5cedd47",
   "metadata": {},
   "source": [
    "# Local Manga AI\n",
    "\n",
    "This notebook launches a fully-local pipeline: **Qwen2.5 → SDXL → Page Composer** and can optionally expose a public HTTPS URL via **Cloudflare Tunnel**.\n",
    "\n",
    "Model folders:\n",
    "- `models/qwen2.5/`\n",
    "- `models/sdxl/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cafbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Runtime settings (run this BEFORE loading any torch/diffusers models)\n",
    "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True\")\n",
    "\n",
    "print(\"PYTORCH_CUDA_ALLOC_CONF=\", os.environ.get(\"PYTORCH_CUDA_ALLOC_CONF\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca83190b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Detect paths\n",
    "CWD = Path.cwd().resolve()\n",
    "ROOT = CWD\n",
    "\n",
    "# If we opened this notebook from inside ./manga_ai, ROOT is already the package folder.\n",
    "# If we opened it from repo root, ROOT needs to be ./manga_ai.\n",
    "if not ((ROOT / \"models\").exists() and (ROOT / \"scripts\").exists()):\n",
    "    if (ROOT / \"manga_ai\" / \"models\").exists() and (ROOT / \"manga_ai\" / \"scripts\").exists():\n",
    "        ROOT = (ROOT / \"manga_ai\").resolve()\n",
    "\n",
    "REPO_ROOT = ROOT.parent\n",
    "\n",
    "# Make sure we can `import manga_ai` regardless of where Jupyter was launched from.\n",
    "if str(REPO_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(REPO_ROOT))\n",
    "\n",
    "print(\"CWD:\", CWD)\n",
    "print(\"ROOT (manga_ai folder):\", ROOT)\n",
    "print(\"REPO_ROOT:\", REPO_ROOT)\n",
    "print(\"Python:\", sys.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e2bc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from manga_ai.scripts.model_downloader import ensure_models_downloaded\n",
    "from manga_ai.scripts.pipeline import default_model_paths\n",
    "\n",
    "paths = default_model_paths(ROOT)\n",
    "print(\"Qwen dir:\", paths.qwen_dir)\n",
    "print(\"SDXL dir:\", paths.sdxl_dir)\n",
    "\n",
    "# This will download models into the folders above if they are missing.\n",
    "# For gated HuggingFace models, set: MANGA_AI_HF_TOKEN\n",
    "try:\n",
    "    ensure_models_downloaded(\n",
    "        qwen_dir=paths.qwen_dir,\n",
    "        sdxl_dir=paths.sdxl_dir,\n",
    "        hf_token=os.environ.get(\"MANGA_AI_HF_TOKEN\"),\n",
    "    )\n",
    "    print(\"Models are present (downloaded if needed).\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\n",
    "        \"Model download/setup failed. If SDXL/Qwen are gated, accept the license on HuggingFace and set MANGA_AI_HF_TOKEN. \"\n",
    "        f\"Original error: {e}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3742eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Quick sanity checks (doesn't load the full ML pipelines)\n",
    "assert (paths.qwen_dir / \"config.json\").exists(), \"Qwen config.json not found\"\n",
    "assert (paths.sdxl_dir / \"model_index.json\").exists(), \"SDXL model_index.json not found\"\n",
    "\n",
    "print(\"Sanity checks passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e91ef0c",
   "metadata": {},
   "source": [
    "## Launch Web UI + Cloudflare Public URL\n",
    "\n",
    "Running the next cell will:\n",
    "- Start Gradio locally on port `7860`\n",
    "- Start a Cloudflare tunnel and print a public `https://...trycloudflare.com` URL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8e1170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import socket\n",
    "import tempfile\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "\n",
    "import gradio as gr\n",
    "import torch\n",
    "from diffusers import (DDIMScheduler, DPMSolverMultistepScheduler, EulerAncestralDiscreteScheduler,\n",
    "                       EulerDiscreteScheduler, LMSDiscreteScheduler, PNDMScheduler,\n",
    "                       StableDiffusionXLImg2ImgPipeline, StableDiffusionXLPipeline, UniPCMultistepScheduler)\n",
    "from PIL import Image\n",
    "\n",
    "from manga_ai.scripts.cloudflare_tunnel import start_tunnel\n",
    "\n",
    "HOST = \"127.0.0.1\"\n",
    "\n",
    "# Pick a free port\n",
    "with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "    s.bind((HOST, 0))\n",
    "    PORT = int(s.getsockname()[1])\n",
    "\n",
    "MODEL_DIR = Path(ROOT) / \"models\" / \"sdxl\"  # Animagine XL 4.0 is stored here\n",
    "\n",
    "print(f\"=== MODEL DIRECTORY ===\")\n",
    "print(f\"Looking for Animagine XL 4.0 at: {MODEL_DIR}\")\n",
    "print(f\"Directory exists: {MODEL_DIR.exists()}\")\n",
    "if MODEL_DIR.exists():\n",
    "    files = list(MODEL_DIR.glob(\"*\"))[:10]  # Show first 10 files\n",
    "    print(f\"Files in model dir: {[f.name for f in files]}\")\n",
    "else:\n",
    "    print(\"ERROR: Model directory not found!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "dtype = torch.float16\n",
    "_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"=== DEVICE INFO ===\")\n",
    "print(f\"Using device: {_device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "_pipe = None\n",
    "_img2img = None\n",
    "\n",
    "def _create_dpmpp_2m_sde_config():\n",
    "    \"\"\"Create the exact DPM++ 2M SDE configuration.\"\"\"\n",
    "    print(\"\\n=== DPM++ 2M SDE CONFIGURATION ===\")\n",
    "    config = {\n",
    "        # Core DPM++ 2M settings\n",
    "        \"algorithm_type\": \"dpmsolver++\",\n",
    "        \"solver_type\": \"midpoint\",  # This makes it \"2M\" (multistep)\n",
    "        \n",
    "        # SDE (stochastic differential equation) settings\n",
    "        \"variance_type\": \"fixed_small\",  # SDE style (Euler a-like)\n",
    "        \"prediction_type\": \"epsilon\",\n",
    "        \n",
    "        # Karras sigmas for better quality\n",
    "        \"use_karras_sigmas\": True,\n",
    "        \n",
    "        # Additional DPM++ 2M SDE parameters\n",
    "        \"thresholding\": False,\n",
    "        \"dynamic_thresholding_ratio\": 0.995,\n",
    "        \"sample_max_value\": 1.0,\n",
    "        \"lower_order_final\": True,\n",
    "        \"lambda_min_clipped\": -float(\"inf\"),\n",
    "        \n",
    "        # SDE specific noise handling\n",
    "        \"noise_offset\": 0,\n",
    "        \"rho\": 7.0,  # Karras rho parameter\n",
    "    }\n",
    "    \n",
    "    print(\"DPM++ 2M SDE Configuration:\")\n",
    "    for key, value in config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    return config\n",
    "\n",
    "def _make_dpmpp_2m_sde_scheduler():\n",
    "    \"\"\"Create DPM++ 2M SDE scheduler with comprehensive verification.\"\"\"\n",
    "    print(\"\\n=== CREATING DPM++ 2M SDE SCHEDULER ===\")\n",
    "    print(\"Attempting to create DPM++ 2M SDE (Euler a style)...\")\n",
    "    \n",
    "    # Get the exact configuration\n",
    "    config = _create_dpmpp_2m_sde_config()\n",
    "    \n",
    "    try:\n",
    "        # Method 1: Try to create from config directly\n",
    "        print(\"Method 1: Creating from config...\")\n",
    "        scheduler = DPMSolverMultistepScheduler.from_config(config)\n",
    "        print(\"✓ SUCCESS: DPM++ 2M SDE created from config\")\n",
    "        return scheduler\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Method 1 failed: {str(e)[:200]}\")\n",
    "        \n",
    "        try:\n",
    "            # Method 2: Try to load from model directory then override config\n",
    "            print(\"Method 2: Loading from model directory...\")\n",
    "            scheduler = DPMSolverMultistepScheduler.from_pretrained(\n",
    "                MODEL_DIR,\n",
    "                subfolder=\"scheduler\",\n",
    "                use_karras_sigmas=True,\n",
    "            )\n",
    "            \n",
    "            # Override with DPM++ 2M SDE settings\n",
    "            print(\"Overriding configuration...\")\n",
    "            for key, value in config.items():\n",
    "                if hasattr(scheduler.config, key):\n",
    "                    setattr(scheduler.config, key, value)\n",
    "                    print(f\"  Set {key}: {value}\")\n",
    "            \n",
    "            print(\"✓ SUCCESS: DPM++ 2M SDE created from model + override\")\n",
    "            return scheduler\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\"✗ Method 2 failed: {str(e2)[:200]}\")\n",
    "            \n",
    "            try:\n",
    "                # Method 3: Create generic DPMSolverMultistepScheduler and force settings\n",
    "                print(\"Method 3: Creating generic scheduler...\")\n",
    "                scheduler = DPMSolverMultistepScheduler.from_config({})\n",
    "                \n",
    "                # Force all DPM++ 2M SDE settings\n",
    "                print(\"Force-setting configuration...\")\n",
    "                for key, value in config.items():\n",
    "                    try:\n",
    "                        setattr(scheduler.config, key, value)\n",
    "                        print(f\"  Forced {key}: {value}\")\n",
    "                    except Exception as e3:\n",
    "                        print(f\"  Could not set {key}: {str(e3)[:100]}\")\n",
    "                \n",
    "                print(\"✓ SUCCESS: Generic scheduler configured as DPM++ 2M SDE\")\n",
    "                return scheduler\n",
    "                \n",
    "            except Exception as e3:\n",
    "                print(f\"✗ Method 3 failed: {str(e3)[:200]}\")\n",
    "                print(\"All DPM++ 2M SDE attempts failed, using fallback...\")\n",
    "                return None\n",
    "\n",
    "def _verify_dpmpp_2m_sde_configuration(scheduler):\n",
    "    \"\"\"Comprehensive verification of DPM++ 2M SDE configuration.\"\"\"\n",
    "    print(\"\\n=== VERIFYING DPM++ 2M SDE CONFIGURATION ===\")\n",
    "    \n",
    "    if not hasattr(scheduler, 'config'):\n",
    "        print(\"✗ ERROR: Scheduler has no config attribute\")\n",
    "        return False\n",
    "    \n",
    "    config = scheduler.config\n",
    "    scheduler_name = scheduler.__class__.__name__\n",
    "    \n",
    "    print(f\"Scheduler class: {scheduler_name}\")\n",
    "    print(\"\\nConfiguration verification:\")\n",
    "    \n",
    "    # Required settings for DPM++ 2M SDE\n",
    "    required_settings = {\n",
    "        \"algorithm_type\": \"dpmsolver++\",\n",
    "        \"solver_type\": \"midpoint\", \n",
    "        \"variance_type\": \"fixed_small\",\n",
    "        \"use_karras_sigmas\": True,\n",
    "    }\n",
    "    \n",
    "    all_correct = True\n",
    "    for setting, expected_value in required_settings.items():\n",
    "        actual_value = getattr(config, setting, None)\n",
    "        status = \"✓\" if actual_value == expected_value else \"✗\"\n",
    "        print(f\"  {status} {setting}: {actual_value} (expected: {expected_value})\")\n",
    "        if actual_value != expected_value:\n",
    "            all_correct = False\n",
    "    \n",
    "    # Optional but recommended settings\n",
    "    optional_settings = {\n",
    "        \"prediction_type\": \"epsilon\",\n",
    "        \"thresholding\": False,\n",
    "        \"lower_order_final\": True,\n",
    "    }\n",
    "    \n",
    "    print(\"\\nOptional settings:\")\n",
    "    for setting, expected_value in optional_settings.items():\n",
    "        actual_value = getattr(config, setting, None)\n",
    "        status = \"✓\" if actual_value == expected_value else \"⚠\"\n",
    "        print(f\"  {status} {setting}: {actual_value} (expected: {expected_value})\")\n",
    "    \n",
    "    # Final verification result\n",
    "    if all_correct:\n",
    "        print(\"\\n✓ VERIFICATION PASSED: Using DPM++ 2M SDE (Euler a style)\")\n",
    "        return True\n",
    "    elif \"dpmsolver++\" in str(getattr(config, 'algorithm_type', '')):\n",
    "        print(\"\\n⚠ PARTIAL VERIFICATION: DPM++ detected but not full 2M SDE\")\n",
    "        return False\n",
    "    else:\n",
    "        print(f\"\\n✗ VERIFICATION FAILED: Not DPM++ 2M SDE (using {scheduler_name})\")\n",
    "        return False\n",
    "\n",
    "def _make_dpmpp_2m_sde_scheduler():\n",
    "    \"\"\"Create DPM++ 2M SDE scheduler with comprehensive verification.\"\"\"\n",
    "    print(\"\\n=== CREATING DPM++ 2M SDE SCHEDULER ===\")\n",
    "    print(\"Attempting to create DPM++ 2M SDE (Euler a style)...\")\n",
    "    \n",
    "    # Get the exact configuration\n",
    "    config = _create_dpmpp_2m_sde_config()\n",
    "    \n",
    "    try:\n",
    "        # Method 1: Try to create from config directly\n",
    "        print(\"Method 1: Creating from config...\")\n",
    "        scheduler = DPMSolverMultistepScheduler.from_config(config)\n",
    "        print(\"✓ SUCCESS: DPM++ 2M SDE created from config\")\n",
    "        return scheduler\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Method 1 failed: {str(e)[:200]}\")\n",
    "        \n",
    "        try:\n",
    "            # Method 2: Try to load from model directory then override config\n",
    "            print(\"Method 2: Loading from model directory...\")\n",
    "            scheduler = DPMSolverMultistepScheduler.from_pretrained(\n",
    "                MODEL_DIR,\n",
    "                subfolder=\"scheduler\",\n",
    "                use_karras_sigmas=True,\n",
    "            )\n",
    "            \n",
    "            # Override with DPM++ 2M SDE settings\n",
    "            print(\"Overriding configuration...\")\n",
    "            for key, value in config.items():\n",
    "                if hasattr(scheduler.config, key):\n",
    "                    setattr(scheduler.config, key, value)\n",
    "                    print(f\"  Set {key}: {value}\")\n",
    "            \n",
    "            print(\"✓ SUCCESS: DPM++ 2M SDE created from model + override\")\n",
    "            return scheduler\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\"✗ Method 2 failed: {str(e2)[:200]}\")\n",
    "            \n",
    "            try:\n",
    "                # Method 3: Create generic DPMSolverMultistepScheduler and force settings\n",
    "                print(\"Method 3: Creating generic scheduler...\")\n",
    "                scheduler = DPMSolverMultistepScheduler.from_config({})\n",
    "                \n",
    "                # Force all DPM++ 2M SDE settings\n",
    "                print(\"Force-setting configuration...\")\n",
    "                for key, value in config.items():\n",
    "                    try:\n",
    "                        setattr(scheduler.config, key, value)\n",
    "                        print(f\"  Forced {key}: {value}\")\n",
    "                    except Exception as e3:\n",
    "                        print(f\"  Could not set {key}: {str(e3)[:100]}\")\n",
    "                \n",
    "                print(\"✓ SUCCESS: Generic scheduler configured as DPM++ 2M SDE\")\n",
    "                return scheduler\n",
    "                \n",
    "            except Exception as e3:\n",
    "                print(f\"✗ Method 3 failed: {str(e3)[:200]}\")\n",
    "                print(\"All DPM++ 2M SDE attempts failed, using fallback...\")\n",
    "                return None\n",
    "\n",
    "def _get_pipes():\n",
    "    global _pipe, _img2img\n",
    "    print(\"\\n=== LOADING PIPELINES ===\")\n",
    "    \n",
    "    if _pipe is None:\n",
    "        print(\"Loading txt2img pipeline...\")\n",
    "        try:\n",
    "            _pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "                MODEL_DIR,\n",
    "                torch_dtype=dtype,\n",
    "                use_safetensors=True,\n",
    "                variant=None,\n",
    "            )\n",
    "            print(\"✓ Pipeline loaded successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ FAILED to load pipeline: {str(e)}\")\n",
    "            print(\"\\nFull error:\")\n",
    "            traceback.print_exc()\n",
    "            raise\n",
    "        \n",
    "        print(\"Creating DPM++ 2M SDE scheduler...\")\n",
    "        scheduler = _make_dpmpp_2m_sde_scheduler()\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            _pipe.scheduler = scheduler\n",
    "            if _verify_dpmpp_2m_sde_configuration(scheduler):\n",
    "                print(\"✓ DPM++ 2M SDE scheduler verified and active\")\n",
    "            else:\n",
    "                print(\"⚠ DPM++ 2M SDE scheduler created but verification failed\")\n",
    "        else:\n",
    "            print(\"✗ DPM++ 2M SDE failed, using fallback scheduler\")\n",
    "            # Use fallback\n",
    "            fallback_schedulers = [\n",
    "                (\"Euler Ancestral\", lambda: EulerAncestralDiscreteScheduler.from_config({\n",
    "                    \"use_karras_sigmas\": True,\n",
    "                    \"prediction_type\": \"epsilon\",\n",
    "                    \"steps_offset\": 0,\n",
    "                })),\n",
    "                (\"Euler Discrete\", lambda: EulerDiscreteScheduler.from_config({\n",
    "                    \"use_karras_sigmas\": True,\n",
    "                    \"prediction_type\": \"epsilon\",\n",
    "                    \"steps_offset\": 1,\n",
    "                })),\n",
    "                (\"PNDMScheduler\", lambda: PNDMScheduler.from_config({})),\n",
    "            ]\n",
    "            \n",
    "            for name, scheduler_fn in fallback_schedulers:\n",
    "                try:\n",
    "                    _pipe.scheduler = scheduler_fn()\n",
    "                    print(f\"✓ Fallback: Using {name}\")\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(f\"✗ Fallback {name} failed: {str(e)[:100]}\")\n",
    "                    continue\n",
    "        \n",
    "        _pipe.set_progress_bar_config(disable=True)\n",
    "        \n",
    "        print(\"Moving to device and optimizing...\")\n",
    "        if _device == \"cuda\":\n",
    "            _pipe.to(_device)\n",
    "            try:\n",
    "                _pipe.enable_xformers_memory_efficient_attention()\n",
    "                print(\"✓ xFormers enabled\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠ xFormers failed: {str(e)[:100]}\")\n",
    "            try:\n",
    "                if hasattr(_pipe, \"vae\") and hasattr(_pipe.vae, \"enable_tiling\"):\n",
    "                    _pipe.vae.enable_tiling()\n",
    "                else:\n",
    "                    _pipe.enable_vae_tiling()\n",
    "                print(\"✓ VAE tiling enabled\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠ VAE tiling failed: {str(e)[:100]}\")\n",
    "        \n",
    "        print(\"✓ txt2img pipeline ready\")\n",
    "\n",
    "    if _img2img is None:\n",
    "        print(\"Loading img2img pipeline...\")\n",
    "        try:\n",
    "            _img2img = StableDiffusionXLImg2ImgPipeline.from_pretrained(\n",
    "                MODEL_DIR,\n",
    "                torch_dtype=dtype,\n",
    "                use_safetensors=True,\n",
    "                variant=None,\n",
    "            )\n",
    "            print(\"✓ img2img pipeline loaded successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ FAILED to load img2img pipeline: {str(e)}\")\n",
    "            print(\"\\nFull error:\")\n",
    "            traceback.print_exc()\n",
    "            raise\n",
    "        \n",
    "        _img2img.scheduler = _pipe.scheduler\n",
    "        _img2img.set_progress_bar_config(disable=True)\n",
    "        \n",
    "        if _device == \"cuda\":\n",
    "            _img2img.to(_device)\n",
    "            try:\n",
    "                _img2img.enable_xformers_memory_efficient_attention()\n",
    "            except Exception:\n",
    "                pass\n",
    "            try:\n",
    "                if hasattr(_img2img, \"vae\") and hasattr(_img2img.vae, \"enable_tiling\"):\n",
    "                    _img2img.vae.enable_tiling()\n",
    "                else:\n",
    "                    _img2img.enable_vae_tiling()\n",
    "            except Exception:\n",
    "                pass\n",
    "        \n",
    "        print(\"✓ img2img pipeline ready\")\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "    return _pipe, _img2img\n",
    "\n",
    "\n",
    "def generate(\n",
    "    prompt: str,\n",
    "    negative: str,\n",
    "    width: int,\n",
    "    height: int,\n",
    "    steps: int,\n",
    "    cfg: float,\n",
    "    seed: int,\n",
    "    hires: bool,\n",
    "    hires_upscale: float,\n",
    "    hires_strength: float,\n",
    "    hires_steps: int,\n",
    "):\n",
    "    print(f\"\\n=== GENERATING IMAGE ===\")\n",
    "    print(f\"Prompt: {prompt[:100]}...\")\n",
    "    print(f\"Size: {width}x{height}, Steps: {steps}, CFG: {cfg}, Seed: {seed}\")\n",
    "    print(f\"Hires: {hires}, Upscale: {hires_upscale}, Strength: {hires_strength}\")\n",
    "    \n",
    "    try:\n",
    "        pipe, img2img = _get_pipes()\n",
    "    except Exception as e:\n",
    "        print(f\"✗ PIPELINE ERROR: {str(e)}\")\n",
    "        print(\"\\nFull error:\")\n",
    "        traceback.print_exc()\n",
    "        raise gr.Error(f\"Pipeline loading failed: {str(e)}\")\n",
    "\n",
    "    width = int(width) // 8 * 8\n",
    "    height = int(height) // 8 * 8\n",
    "\n",
    "    g = torch.Generator(device=_device).manual_seed(int(seed))\n",
    "\n",
    "    if not hires:\n",
    "        print(\"Generating single pass...\")\n",
    "        try:\n",
    "            img = pipe(\n",
    "                prompt=prompt,\n",
    "                negative_prompt=negative,\n",
    "                width=width,\n",
    "                height=height,\n",
    "                num_inference_steps=int(steps),\n",
    "                guidance_scale=float(cfg),\n",
    "                generator=g,\n",
    "            ).images[0]\n",
    "            print(\"✓ Generation complete\")\n",
    "            return img\n",
    "        except Exception as e:\n",
    "            print(f\"✗ GENERATION ERROR: {str(e)}\")\n",
    "            print(\"\\nFull error:\")\n",
    "            traceback.print_exc()\n",
    "            raise gr.Error(f\"Generation failed: {str(e)}\")\n",
    "\n",
    "    print(\"Generating 2-pass hires...\")\n",
    "    try:\n",
    "        # Pass 1: Base generation\n",
    "        base_w = max(1024, int(width / float(hires_upscale)))\n",
    "        base_h = max(1024, int(height / float(hires_upscale)))\n",
    "        base_w = base_w // 8 * 8\n",
    "        base_h = base_h // 8 * 8\n",
    "        \n",
    "        print(f\"Pass 1: {base_w}x{base_h}\")\n",
    "        img = pipe(\n",
    "            prompt=prompt,\n",
    "            negative_prompt=negative,\n",
    "            width=base_w,\n",
    "            height=base_h,\n",
    "            num_inference_steps=int(steps),\n",
    "            guidance_scale=float(cfg),\n",
    "            generator=g,\n",
    "        ).images[0]\n",
    "\n",
    "        if not isinstance(img, Image.Image):\n",
    "            img = Image.fromarray(img)\n",
    "\n",
    "        # Upscale\n",
    "        img = img.resize((width, height), resample=Image.LANCZOS)\n",
    "        \n",
    "        # Pass 2: Refine\n",
    "        print(f\"Pass 2: {width}x{height}\")\n",
    "        img = img2img(\n",
    "            prompt=prompt,\n",
    "            negative_prompt=negative,\n",
    "            image=img,\n",
    "            strength=float(hires_strength),\n",
    "            num_inference_steps=int(hires_steps),\n",
    "            guidance_scale=float(cfg),\n",
    "            generator=g,\n",
    "        ).images[0]\n",
    "\n",
    "        print(\"✓ 2-pass generation complete\")\n",
    "        return img\n",
    "    except Exception as e:\n",
    "        print(f\"✗ HIRES GENERATION ERROR: {str(e)}\")\n",
    "        print(\"\\nFull error:\")\n",
    "        traceback.print_exc()\n",
    "        raise gr.Error(f\"Hires generation failed: {str(e)}\")\n",
    "\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=generate,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Prompt\", lines=6, value=\"masterpiece, best quality, very aesthetic, absurdres, clean manga lineart, crisp ink, screentone shading, sharp focus, high contrast\"),\n",
    "        gr.Textbox(label=\"Negative\", lines=3, value=\"worst quality, low quality, lowres, blurry, jpeg artifacts, watermark, logo, signature, bad anatomy, bad hands, extra fingers, missing fingers, extra limbs, deformed\"),\n",
    "        gr.Slider(label=\"Width\", minimum=768, maximum=2048, step=64, value=1024),\n",
    "        gr.Slider(label=\"Height\", minimum=768, maximum=3072, step=64, value=1536),\n",
    "        gr.Slider(label=\"Steps\", minimum=10, maximum=120, step=1, value=60),\n",
    "        gr.Slider(label=\"CFG\", minimum=1.0, maximum=12.0, step=0.1, value=6.5),\n",
    "        gr.Number(label=\"Seed\", value=1234, precision=0),\n",
    "        gr.Checkbox(label=\"Hires refine (2-pass)\", value=True),\n",
    "        gr.Slider(label=\"Hires upscale\", minimum=1.5, maximum=3.0, step=0.1, value=2.0),\n",
    "        gr.Slider(label=\"Hires strength\", minimum=0.05, maximum=0.6, step=0.01, value=0.26),\n",
    "        gr.Slider(label=\"Hires steps\", minimum=5, maximum=60, step=1, value=32),\n",
    "    ],\n",
    "    outputs=gr.Image(type=\"pil\", label=\"Result\"),\n",
    "    title=\"Animagine XL 4.0 (Direct) — DPM++ 2M SDE (Euler a)\",\n",
    "    description=\"Using DPM++ 2M SDE sampler (Euler a style) with comprehensive configuration verification\",\n",
    ")\n",
    "\n",
    "print(\"\\n=== STARTING GRADIO ===\")\n",
    "print(f\"Local URL will be: http://{HOST}:{PORT}\")\n",
    "\n",
    "# Launch local UI\n",
    "# NOTE: prevent_thread_lock=True keeps this cell from blocking indefinitely.\n",
    "demo.launch(server_name=HOST, server_port=PORT, share=False, inbrowser=False, prevent_thread_lock=True)\n",
    "\n",
    "print(\"\\n=== STARTING CLOUDFLARE TUNNEL ===\")\n",
    "# Start Cloudflare tunnel (use an executable cache dir; teamspace outputs may be mounted noexec)\n",
    "cloudflared_cache = Path.home() / \".cache\" / \"manga_ai\" / \"cloudflared\"\n",
    "# If home cache isn't usable, fallback to /tmp:\n",
    "# cloudflared_cache = Path(tempfile.gettempdir()) / \"manga_ai\" / \"cloudflared\"\n",
    "\n",
    "try:\n",
    "    tunnel = start_tunnel(local_port=PORT, cache_dir=cloudflared_cache)\n",
    "    print(\"✓ Cloudflare tunnel started\")\n",
    "    print(f\"Public URL: {tunnel.public_url}\")\n",
    "    print(f\"Local URL: http://{HOST}:{PORT}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ TUNNEL ERROR: {str(e)}\")\n",
    "    print(\"\\nFull error:\")\n",
    "    traceback.print_exc()\n",
    "    print(f\"Local URL still available: http://{HOST}:{PORT}\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"READY! You can now generate images.\")\n",
    "print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
